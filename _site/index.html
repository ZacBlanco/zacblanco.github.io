<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">        
  <title>
    
      zacblanco.github.io &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="zacblanco.github.io" href="/atom.xml">
    
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

    <!-- Google Analytics -->
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58703489-1', 'auto');
  ga('send', 'pageview');

</script>
    
</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
            <a href="/" title="Home">zacblanco.github.io</a>

            
            &nbsp;&nbsp;&nbsp;
            <small><a href="/projects">Projects</a></small>
            
            &nbsp;&nbsp;&nbsp;
            <small><a href="/archive">Archive</a></small>
            
            &nbsp;&nbsp;&nbsp;
            <small><a href="/about">About</a></small>
            
            &nbsp;&nbsp;&nbsp;
            <small><a href="/atom.xml">Feed</a></small>
            
        </h3>
      </header>

      <main>
        <div class="posts">
  
  <article class="post">
    <h1 class="post-title">
      <a href="/nerual-networks-and-backpropagation.html">
        Neural Networks and the Backpropagation Algorithm
      </a>
    </h1>

    <time datetime="2015-05-09T20:12:56-04:00" class="post-date">09 May 2015</time>

    <p>The other day I had been looking up information on <a href="http://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. I’m new to some of the topics in this field but I have been introduced to some before. I’ve always wanted to learn more. The idea of teaching a computer how to “learn” just seems intriguing to me.</p>

<p>Specifically I had been researching <a href="http://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a>. They are useful because they can help uncover hidden patterns in a plethora of data, or be taught to perform certain tasks such as handwriting or facial recognition.</p>

<p>I had watched an <a href="https://www.youtube.com/watch?v=q0pm3BrIUFo">MIT OpenCourseWare Lecture</a> which gave a rough introduction to neural networks which I found to be quite captivating. I found it so fantastic actually that I wanted to share the basic concepts here. For the purpose of this post, I’m going to assume that we’re teaching this neural net via supervised learning.</p>

<p>Or, in other words, we are going to make this neural network learn by giving it a set of “correct” input and output values, which it will use to calibrate, or change its own structure to make sure that it also outputs the correct values. Now let’s dive in!</p>

<p>To explain briefly, neural networks are modeled after the same neurons that exist in our brain.</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/500px-Neuron.svg.png" alt="Image of Neuron" /></p>

<p>For our purpose we focus on only a few of the main features of a neuron which are critical to our model for a neural network.</p>

<p>The main parts of the neuron we want to focus on are:</p>

<ul>
  <li>Dendrites</li>
  <li>The Axon</li>
  <li>The Axon Terminals</li>
</ul>

<p><strong>The dendrites</strong> are small tree-like structures that are stimulated when enough neurotransmitters are dumped into the synapse between a neuron and the end of the dendrite. Some of these dendrites might be easier to stimulate than other based on various factors</p>

<p><strong>The axon</strong> is the lengthy part of the neuron which simply propagates a “spike” of energy to the axon terminals, which originates from the dendrites.</p>

<p><strong>The axon terminals</strong> dump neurotransmitters into the synapse to help stimulate the dendrites of the next neuron.</p>

<p>If there are enough neurotransmitters dumped into the synapse, It will stimulate the next neuron’s dendrites, sending a spike of “excitement” or “energy” to the next neuron, and so forth. At each end of the axom terminals there is another neuron’s dendrites separated by a space called the <strong>synapse</strong>. If a neuron is stimulated, it will dump neurotransmitters into the synapse.</p>

<p>This is a gross simplification of a neuron, but for the model of a neural network and the backpropagation algorithm, we don’t need to understand more than this.</p>

<h4 id="modeling-the-neuron">Modeling the Neuron</h4>
<hr />

<p><img src="../assets/images/neural-nets/net-model.jpg" alt="Image of Model" /></p>

<p>First, notice the inputs. Each input from <script type="math/tex">x_1</script> to <script type="math/tex">x_n</script> is given as an input to the neuron. Each of the inputs is multiplied by a weight <script type="math/tex">w</script> corresponding to an input before being summed into the function.</p>

<p>The neuron also has a bias, labeled  <script type="math/tex">b</script> in the diagram, which I like to call <script type="math/tex">w_0</script>. We can think of this bias as a given input to each neuron that always has a value of -1. The weight of the bias input <script type="math/tex">w_0</script> can be adjusted accordingly.</p>

<p>Think of this part as the neurotransmitters being the inputs multiplied by the weights which stimulate the dendrites of the neuron.</p>

<p>From here we can represent the total stimulus as the sum of each input multiplied by a corresponding weight.</p>

<script type="math/tex; mode=display">-w_0 + \sum_{i=1}^{n}x_iw_i</script>

<p>The value of this will be then be passed onto the activation, or <em>threshold</em> function. The threshold function might be best viewed as a single-step function, where say once the input <script type="math/tex">\alpha</script> reaches a certain value, it will output a certain value. We’re going to keep this to ones and zeros.</p>

<p><img src="../assets/images/neural-nets/threshold-med.png" alt="Threshold Example" /></p>

<p>Next, once the threshold is activated or not, we will get an output from this neuron. Once we run this neural network through for the first time, we are going to need to find out how good (or bad) the neural network did at predicting our answer.</p>

<p>Remember how I said that the neural network is given a set of “correct” inputs and outputs to learn from? Well here’s where that comes into play.</p>

<p>Let’s say that our output of the neural net is <script type="math/tex">y</script> and our desired or <em>correct</em> output is going to be assigned to <script type="math/tex">d</script>. We are going to need a function to test our performance of the neural network.</p>

<p>Let’s call this function <script type="math/tex">P</script>. Also, for simplicity’s sake, and because this is <em>convenient</em> we’re going to make the performance function the following:</p>

<script type="math/tex; mode=display">P(d, y) = -\frac{1}{2}(d-y)^2</script>

<p>(<em>You’ll see what I mean by convenient later</em>)</p>

<h4 id="adjusting-the-model">Adjusting the Model</h4>
<hr />

<p>So now we’ve got this model. We have a function to test whether our output is close to what we want it to be. We have a way to add our inputs together. What comes next?</p>

<p>Well, we need to have some way of adjusting the neuron to make sure we get closer to our desired output. But how do we know what to adjust? and by how much?</p>

<p>We’re going to use a method called <strong>gradient descent</strong> (or <em>ascent</em> depending on direction). We’re going to use some math here.</p>

<p>So we know that our desired outputs <script type="math/tex">d</script> can be given by <script type="math/tex">\overline{d} = g(\overline{x})</script>; where <script type="math/tex">\overline{d}</script> and <script type="math/tex">\overline{x}</script> are our vectors of inputs and desired outputs that our neural network is going to learn from.</p>

<p>The problem is that we don’t know what function <script type="math/tex">g</script> is, which is why we’re using the neural network to represent it. With the neural networks, we can represent our inputs and outputs by a multivaraite function <script type="math/tex">f</script>. This function can be represented by <script type="math/tex">\overline{y} = f(\overline{x}, \overline{w})</script>.</p>

<p>Given these functions, we can see our performance function is now</p>

<script type="math/tex; mode=display">P(g(\overline{x}), f(\overline{x}, \overline{w})) = -\frac{1}{2}(g(\overline{x})-f(\overline{x}, \overline{w}))^2</script>

<p>We can see from here the performance function is really just a function of the inputs, <script type="math/tex">x</script> and the weights <script type="math/tex">w</script>.</p>

<p>Now imagine that we add some complexity to our neural network. We are going make two neural layers <em>chained</em> together. That is, we have an input <script type="math/tex">x</script> passed to <script type="math/tex">w_1</script> passed to a threshold function, who’s output is then the input to the next neuron. This is multiplied by another weight <script type="math/tex">w_n</script> which is then passed to the next neuron’s threshold. The output of this 2nd threshold will be the output of the neural network.</p>

<p>Just imagine the above diagram of the neural network laid end to end and on top of each other. A general overview can be seen below, where each circle represents a neuron:</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" alt="Model of Neural Net" /></p>

<p>This is where things start to become more interesting. Using the algorithm of gradient descent, we can alter the weights <script type="math/tex">\overline{w}</script> by taking the gradient of our performance function with respect to each weight. We can also multiply this gradient by a rate constant, let’s say <script type="math/tex">\delta</script>, that will determine how much we alter the weights over each iteration.</p>

<p>So let’s look at our formula</p>

<script type="math/tex; mode=display">\triangle w = \delta\nabla_w P = \delta\langle \frac{\partial P}{\partial w_0}, \frac{\partial P}{\partial w_1}, \cdot\cdot\cdot, \frac{\partial P}{\partial w_n} \rangle</script>

<p>This is now telling us that the change in our <script type="math/tex">w</script> over each iteration is going to be equal to <script type="math/tex">\delta</script> multiplied by the partial derivative of <script type="math/tex">P</script> with respect to each weight.</p>

<p>But wait! We have a problem here. We can’t just simply take the partial derivative of <script type="math/tex">w</script> with respect to <script type="math/tex">P</script> because <script type="math/tex">w</script> lies within our other function <script type="math/tex">f</script>. This means that we are going to need the chain rule.</p>

<p>So this brings up another problem: because we are going to need to take all these derivatives all of the functions in our neural network <em>must</em> be differentiable.</p>

<p>So the problem of differentiability lies within our step function. Because a step function is not differentiable we need another function to replace this. Fortunately there is one similar to it that <em>is</em> differentiable!</p>

<p>It’s called a sigmoid function. You might have seen it before. It has the form</p>

<script type="math/tex; mode=display">S(\alpha) = \frac{1}{1 + e^{-\alpha}}</script>

<p>The graph of the sigmoid has the following shape:</p>

<p><img src="../assets/images/neural-nets/sigmoid.png" alt="Sigmoid function" />.</p>

<p>Great! So where do we go from here?</p>

<p>Well, I suggest you watch the <a href="https://www.youtube.com/watch?v=q0pm3BrIUFo">video I linked earlier</a> if you want a really good understanding of how this works. I will do my best to explain it, but I won’t go into explicit detail of how this all works together. The video will explain things more thoroughly.</p>

<p>So if we keep taking our partial derivatives with the chain rule, it turns out that we will need to take the derivative of this sigmoid function.</p>

<p>Turns out this function is actually pretty neat. Let’s call the sigmoid function <script type="math/tex">\beta</script>. It just so happens, that if you take the derivative of this function <em>and</em> we manipulate it a bit, that the derivative of the function turns out to be expressed in terms of the original function itself. I’ll let the math do the talking here.</p>

<script type="math/tex; mode=display">\beta = \frac{1}{1 + e^{-\alpha}}</script>

<script type="math/tex; mode=display">\frac{d}{d\alpha}(\beta) = (1 + e^{-\alpha})^2 e^{-\alpha}</script>

<p>Then we rearrange our derivative a little bit:</p>

<script type="math/tex; mode=display">\frac{d}{d\alpha}(\beta) = \beta(1-\beta)</script>

<p>Voila! The derivative of the sigmoid function turns out to be given in terms of the original! I find that to be quite astounding. If you don’t believe me you’ll just have to take my word for it. I promise you it works. (This is also why I called this function <em>convenient</em>).</p>

<p>So now if we do the calculation for each of the partial derivative we get something like. Also, pretend like the value <script type="math/tex">y_n</script> is the respective output for each neuron in the neural network and <script type="math/tex">z</script> is the final output  of the neural net.</p>

<p>The change in weight is then given by</p>

<script type="math/tex; mode=display">\frac{\partial P}{\partial w_n} = \delta (w_{n+1})(d - z) (\beta(y_n)(1 - \beta(y_n)) (\beta(y_{n-1})(1 - \beta(y_{n-1}))...</script>

<h4 id="wrapping-up">Wrapping Up</h4>
<hr />

<p>So what does all of this mean?</p>

<p>What this means is that when we are calculating exactly how to adjust the weights in our neural network, the change in each weight depends on a few things.</p>

<ul>
  <li>The input to the neuron</li>
  <li>The output of the neuron</li>
  <li>The weight of the next neuron.</li>
</ul>

<p>This helps explain why it’s called the backpropagation algorithm. You need to calculate the change in weights at the end of your neural network before you can calculate the change in the beginning. So you work your way back in a wave-like manner, hence the name <em>backpropagation</em>.</p>

<p>The other great thing about this algorithm is that the change in weight depends on things you’ve already calculated, <em>or</em> things you’re going to have to calculate anyways. It’s extremely effecient. There are few, if any, wasted computations.</p>

<p>And that’s simply it. The most basic of neural networks and the backpropagation</p>

<hr />


  </article>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2015-05-09T20:12:56-04:00">2015</time>. All rights reserved.
        </small>
      </footer>
    </div>

  </body>
</html>
